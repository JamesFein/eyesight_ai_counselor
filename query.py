"""
é—®ç­”è„šæœ¬
å¯¹ç”¨æˆ·è¾“å…¥çš„é—®é¢˜è¿›è¡Œæ”¹å†™å’Œæ£€ç´¢ï¼Œä½¿ç”¨BM25å’Œè¯­ä¹‰æ··åˆæ£€ç´¢
"""
import asyncio
import logging
from typing import List, Dict, Any
from dotenv import load_dotenv
import os
from pathlib import Path

from qdrant_client import AsyncQdrantClient, models
from qdrant_client.models import SparseVector
from openai import AsyncOpenAI
from pydantic_ai import Agent
import jieba
import hashlib

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
COLLECTION_NAME = "eye_ana_guide"
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
BM25_TOP_K = int(os.getenv("BM25_TOP_K", "5"))
SEMANTIC_TOP_K = int(os.getenv("SEMANTIC_TOP_K", "5"))
BM25_THRESHOLD = float(os.getenv("BM25_THRESHOLD", "0.0"))
SEMANTIC_THRESHOLD = float(os.getenv("SEMANTIC_THRESHOLD", "0.5"))
PROMPTS_DIR = Path(r"C:\Users\Administrator\Desktop\fast-gzmdrw-chat\backend\app\prompts")


class QueryService:
    """é—®ç­”æœåŠ¡ç±»"""
    
    def __init__(self):
        # åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯
        self.qdrant_client = AsyncQdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
            prefer_grpc=os.getenv("QDRANT_PREFER_GRPC", "true").lower() == "true"
        )
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºembeddingï¼‰
        self.openai_client = AsyncOpenAI(
            api_key=os.getenv("EMBEDDING_API_KEY"),
            base_url=os.getenv("EMBEDDING_BASE_URL", "https://api.openai.com/v1")
        )
        
        # åŠ è½½æç¤ºè¯
        self.rewrite_system_prompt = self._load_prompt("rewrite_question_system_prompt.txt")
        self.rewrite_user_prompt_template = self._load_prompt("rewrite_question_user_prompt.txt")
        self.rag_system_prompt = self._load_prompt("rag_agent_system_prompt.txt")
        
        # é…ç½®é—®é¢˜æ”¹å†™ Agentï¼ˆä½¿ç”¨LLMé…ç½®ï¼‰
        os.environ['OPENAI_API_KEY'] = os.getenv("LLM_API_KEY")
        os.environ['OPENAI_BASE_URL'] = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
        
        self.rewrite_agent = Agent(
            model=f'openai:{os.getenv("QUERY_REWRITE_LLM_MODEL", "gpt-4o")}',
            result_type=str,
            system_prompt=self.rewrite_system_prompt,
            model_settings={
                'temperature': float(os.getenv("QUERY_REWRITE_LLM_TEMPERATURE", "0.1")),
                'max_tokens': int(os.getenv("QUERY_REWRITE_LLM_MAX_TOKENS", "300")),
            }
        )
        
        # é…ç½®å›ç­” Agentï¼ˆä½¿ç”¨LLMé…ç½®ï¼‰
        self.answer_agent = Agent(
            model=f'openai:{os.getenv("LLM_MODEL", "gpt-4o-mini")}',
            result_type=str,
            system_prompt=self.rag_system_prompt,
            model_settings={
                'temperature': float(os.getenv("LLM_TEMPERATURE", "0.7")),
                'max_tokens': int(os.getenv("LLM_MAX_TOKENS", "2000")),
            }
        )
        
        logger.info("é—®ç­”æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _load_prompt(self, filename: str) -> str:
        """åŠ è½½æç¤ºè¯æ–‡ä»¶"""
        prompt_path = PROMPTS_DIR / filename
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    
    async def rewrite_question(self, question: str) -> str:
        """æ”¹å†™é—®é¢˜ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— å†å²è®°å½•ï¼‰"""
        logger.info(f"\n{'='*60}")
        logger.info(f"åŸå§‹é—®é¢˜: {question}")
        
        # æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆæ— å†å²è®°å½•ï¼‰
        user_prompt = self.rewrite_user_prompt_template.format(
            history_text="(æ— å†å²è®°å½•)",
            question=question
        )
        
        try:
            result = await self.rewrite_agent.run(user_prompt=user_prompt)
            rewritten = result.data.strip()
            logger.info(f"æ”¹å†™é—®é¢˜: {rewritten}")
            logger.info(f"{'='*60}\n")
            return rewritten
        except Exception as e:
            logger.error(f"é—®é¢˜æ”¹å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜")
            logger.info(f"{'='*60}\n")
            return question
    
    def build_sparse_vector(self, text: str) -> Dict[str, Any]:
        """æ„å»º BM25 ç¨€ç–å‘é‡"""
        if not text.strip():
            return {"indices": [], "values": []}
        
        tokens = list(jieba.cut(text.strip()))
        token_freq = {}
        for token in tokens:
            if token.strip() and len(token.strip()) > 0:
                token_freq[token] = token_freq.get(token, 0) + 1
        
        indices = []
        values = []
        for token, freq in token_freq.items():
            token_bytes = token.encode('utf-8')
            token_hash = int(hashlib.md5(token_bytes).hexdigest()[:8], 16)
            indices.append(token_hash)
            values.append(float(freq))
        
        return {"indices": indices, "values": values}
    
    async def embed_text(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        response = await self.openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text.strip()
        )
        return response.data[0].embedding
    
    async def bm25_search(self, query: str) -> List[Dict[str, Any]]:
        """BM25 æ£€ç´¢"""
        sparse_vector = self.build_sparse_vector(query)
        
        search_results = await self.qdrant_client.query_points(
            collection_name=COLLECTION_NAME,
            query=SparseVector(
                indices=sparse_vector["indices"],
                values=sparse_vector["values"]
            ),
            using="bm25",
            limit=BM25_TOP_K
        )
        
        results = []
        for result in search_results.points:
            if result.score >= BM25_THRESHOLD:
                results.append({
                    "content": result.payload.get("content", ""),
                    "score": result.score,
                    "filename": result.payload.get("filename", ""),
                    "chunk_index": result.payload.get("chunk_index", 0)
                })
        
        return results
    
    async def semantic_search(self, query: str) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æ£€ç´¢"""
        query_vector = await self.embed_text(query)
        
        search_results = await self.qdrant_client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            using="dense",
            limit=SEMANTIC_TOP_K,
            score_threshold=SEMANTIC_THRESHOLD
        )
        
        results = []
        for result in search_results.points:
            results.append({
                "content": result.payload.get("content", ""),
                "score": result.score,
                "filename": result.payload.get("filename", ""),
                "chunk_index": result.payload.get("chunk_index", 0)
            })
        
        return results
    
    async def hybrid_search(self, query: str) -> Dict[str, Any]:
        """æ··åˆæ£€ç´¢"""
        logger.info("ğŸ” å¼€å§‹æ··åˆæ£€ç´¢...")
        
        # å¹¶è¡Œæ‰§è¡ŒBM25å’Œè¯­ä¹‰æ£€ç´¢
        bm25_results, semantic_results = await asyncio.gather(
            self.bm25_search(query),
            self.semantic_search(query)
        )
        
        # å»é‡åˆå¹¶
        seen_contents = set()
        deduplicated = []
        
        for result in bm25_results:
            content = result["content"]
            if content not in seen_contents:
                seen_contents.add(content)
                deduplicated.append(result)
        
        for result in semantic_results:
            content = result["content"]
            if content not in seen_contents:
                seen_contents.add(content)
                deduplicated.append(result)
        
        return {
            "bm25_results": bm25_results,
            "semantic_results": semantic_results,
            "deduplicated_results": deduplicated
        }
    
    def print_retrieval_results(self, results: Dict[str, Any]):
        """æ‰“å°æ£€ç´¢ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š æ£€ç´¢ç»“æœ")
        print("="*80)
        
        print(f"\nğŸ”¤ BM25æ£€ç´¢ç»“æœ ({len(results['bm25_results'])} ä¸ª):")
        for i, result in enumerate(results['bm25_results'], 1):
            print(f"\n[{i}] ç›¸ä¼¼åº¦åˆ†æ•°: {result['score']:.4f}")
            print(f"    æ¥æº: {result['filename']} (å— {result['chunk_index']})")
            print(f"    å†…å®¹: {result['content'][:100]}...")
        
        print(f"\nğŸ¯ è¯­ä¹‰æ£€ç´¢ç»“æœ ({len(results['semantic_results'])} ä¸ª):")
        for i, result in enumerate(results['semantic_results'], 1):
            print(f"\n[{i}] ç›¸ä¼¼åº¦åˆ†æ•°: {result['score']:.4f}")
            print(f"    æ¥æº: {result['filename']} (å— {result['chunk_index']})")
            print(f"    å†…å®¹: {result['content'][:100]}...")
        
        print(f"\nâœ… å»é‡åçš„ç»“æœ ({len(results['deduplicated_results'])} ä¸ª):")
        for i, result in enumerate(results['deduplicated_results'], 1):
            print(f"\n[{i}] ç›¸ä¼¼åº¦åˆ†æ•°: {result['score']:.4f}")
            print(f"    æ¥æº: {result['filename']} (å— {result['chunk_index']})")
            print(f"    å†…å®¹: {result['content'][:100]}...")
        
        print("\n" + "="*80 + "\n")
    
    async def answer_question(self, question: str):
        """å›ç­”é—®é¢˜"""
        # 1. æ”¹å†™é—®é¢˜
        rewritten_question = await self.rewrite_question(question)
        
        # 2. æ··åˆæ£€ç´¢
        retrieval_results = await self.hybrid_search(rewritten_question)
        
        # 3. æ‰“å°æ£€ç´¢ç»“æœ
        self.print_retrieval_results(retrieval_results)
        
        # 4. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(retrieval_results['deduplicated_results'], 1):
            context_parts.append(
                f"[æ–‡æ¡£{i}] æ¥æº: {result['filename']}\n{result['content']}"
            )
        context = "\n\n".join(context_parts)
        
        if not context:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”é—®é¢˜ã€‚\n")
            return
        
        # 5. ç”Ÿæˆå›ç­”
        logger.info("ğŸ’¬ ç”Ÿæˆå›ç­”...")
        user_prompt = f"""å‚è€ƒæ–‡æ¡£ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä»¥ä¸Šå‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        
        try:
            result = await self.answer_agent.run(user_prompt=user_prompt)
            answer = result.data.strip()
            
            print("="*80)
            print("ğŸ’¡ å›ç­”")
            print("="*80)
            print(answer)
            print("\n" + "="*80 + "\n")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}\n")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.qdrant_client.close()
        await self.openai_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    service = QueryService()
    
    try:
        print("\n" + "="*80)
        print("ğŸ¤– çœ¼ç§‘åˆ†æé—®ç­”ç³»ç»Ÿ")
        print("="*80)
        print("è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º\n")
        
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥
            question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['exit', 'quit']:
                print("\nå†è§ï¼ğŸ‘‹\n")
                break
            
            # å›ç­”é—®é¢˜
            await service.answer_question(question)
            
    finally:
        await service.close()


if __name__ == "__main__":
    asyncio.run(main())
