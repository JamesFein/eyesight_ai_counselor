"""
æ•°æ®å¯¼å…¥è„šæœ¬
å°† data ç›®å½•ä¸­çš„ txt æ–‡ä»¶å¯¼å…¥åˆ° Qdrant çš„ eye_ana_guide collection
"""
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
import os

from qdrant_client import AsyncQdrantClient, models
from qdrant_client.models import VectorParams, Distance, PointStruct, SparseVectorParams, Modifier, SparseVector
from openai import AsyncOpenAI
from chonkie import SentenceChunker
import jieba
import hashlib
import uuid

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
COLLECTION_NAME = "eye_ana_guide"
DATA_DIR = Path(r"C:\Users\Administrator\Desktop\fast-gzmdrw-chat\data")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "520"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "50"))
CHUNK_DELIMITERS = ["ã€‚", "ï¼", "ï¼Ÿ", ". ", "! ", "? ", "\n"]
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
EMBEDDING_DIMENSIONS = int(os.getenv("EMBEDDING_DIMENSIONS", "1536"))


class DataIngestion:
    """æ•°æ®å¯¼å…¥ç±»"""
    
    def __init__(self):
        # åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯
        self.qdrant_client = AsyncQdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
            prefer_grpc=os.getenv("QDRANT_PREFER_GRPC", "true").lower() == "true"
        )
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.openai_client = AsyncOpenAI(
            api_key=os.getenv("EMBEDDING_API_KEY"),
            base_url=os.getenv("EMBEDDING_BASE_URL", "https://api.openai.com/v1")
        )
        
        # åˆå§‹åŒ– Chonkie åˆ†å—å™¨
        self.chunker = SentenceChunker(
            tokenizer_or_token_counter="character",
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            delim=CHUNK_DELIMITERS,
            include_delim="prev"
        )
        
        logger.info("æ•°æ®å¯¼å…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    async def ensure_collection_exists(self):
        """ç¡®ä¿ collection å­˜åœ¨"""
        collections = await self.qdrant_client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if COLLECTION_NAME not in collection_names:
            logger.info(f"åˆ›å»º collection: {COLLECTION_NAME}")
            await self.qdrant_client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config={
                    "dense": VectorParams(
                        size=EMBEDDING_DIMENSIONS,
                        distance=Distance.COSINE
                    )
                },
                sparse_vectors_config={
                    "bm25": SparseVectorParams(
                        modifier=Modifier.IDF
                    )
                }
            )
            logger.info(f"Collection {COLLECTION_NAME} åˆ›å»ºæˆåŠŸ")
        else:
            logger.info(f"Collection {COLLECTION_NAME} å·²å­˜åœ¨")
    
    def chunk_text(self, text: str) -> List[str]:
        """ä½¿ç”¨ Chonkie åˆ†å—æ–‡æœ¬"""
        chonks = self.chunker(text)
        chunks = [getattr(c, "text", "").strip() for c in chonks if getattr(c, "text", "").strip()]
        return chunks
    
    def build_sparse_vector(self, text: str) -> Dict[str, Any]:
        """æ„å»º BM25 ç¨€ç–å‘é‡"""
        if not text.strip():
            return {"indices": [], "values": []}
        
        tokens = list(jieba.cut(text.strip()))
        token_freq = {}
        for token in tokens:
            if token.strip() and len(token.strip()) > 0:
                token_freq[token] = token_freq.get(token, 0) + 1
        
        indices = []
        values = []
        for token, freq in token_freq.items():
            token_bytes = token.encode('utf-8')
            token_hash = int(hashlib.md5(token_bytes).hexdigest()[:8], 16)
            indices.append(token_hash)
            values.append(float(freq))
        
        return {"indices": indices, "values": values}
    
    async def embed_text(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        response = await self.openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text.strip()
        )
        return response.data[0].embedding
    
    async def check_file_exists(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨äº collection ä¸­"""
        try:
            scroll_result = await self.qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                scroll_filter=models.Filter(
                    must=[models.FieldCondition(
                        key="filename",
                        match=models.MatchValue(value=filename)
                    )]
                ),
                limit=1,
                with_payload=True,
                with_vectors=False
            )
            return len(scroll_result[0]) > 0
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    async def ingest_file(self, file_path: Path):
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        filename = file_path.name
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if await self.check_file_exists(filename):
            logger.info(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
            return 0
        
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        # åˆ†å—
        chunks = self.chunk_text(content)
        logger.info(f"   åˆ†å—æ•°é‡: {len(chunks)}")
        
        # å¤„ç†æ¯ä¸ªåˆ†å—
        points = []
        for idx, chunk in enumerate(chunks):
            # ç”Ÿæˆå‘é‡
            dense_vector = await self.embed_text(chunk)
            sparse_vector = self.build_sparse_vector(chunk)
            
            # åˆ›å»ºç‚¹
            point = PointStruct(
                id=str(uuid.uuid4()),
                vector={
                    "dense": dense_vector,
                    "bm25": SparseVector(
                        indices=sparse_vector["indices"],
                        values=sparse_vector["values"]
                    )
                },
                payload={
                    "content": chunk,
                    "filename": filename,
                    "chunk_index": idx,
                    "total_chunks": len(chunks),
                    "chunk_size": len(chunk)
                }
            )
            points.append(point)
        
        # æ‰¹é‡ä¸Šä¼ 
        await self.qdrant_client.upsert(
            collection_name=COLLECTION_NAME,
            points=points
        )
        
        logger.info(f"âœ… æˆåŠŸå¯¼å…¥: {filename} ({len(points)} ä¸ªåˆ†å—)")
        return len(points)
    
    async def ingest_all(self):
        """å¯¼å…¥æ‰€æœ‰æ–‡ä»¶"""
        await self.ensure_collection_exists()
        
        txt_files = list(DATA_DIR.glob("*.txt"))
        logger.info(f"å‘ç° {len(txt_files)} ä¸ª txt æ–‡ä»¶")
        
        total_chunks = 0
        for file_path in txt_files:
            chunks_count = await self.ingest_file(file_path)
            total_chunks += chunks_count
        
        logger.info(f"\nğŸ‰ å¯¼å…¥å®Œæˆï¼æ€»å…±å¯¼å…¥ {total_chunks} ä¸ªæ–‡æœ¬å—")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.qdrant_client.close()
        await self.openai_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    ingestion = DataIngestion()
    try:
        await ingestion.ingest_all()
    finally:
        await ingestion.close()


if __name__ == "__main__":
    asyncio.run(main())
