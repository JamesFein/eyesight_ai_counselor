"""
æ•°æ®å¯¼å…¥è„šæœ¬
å°† data ç›®å½•ä¸­çš„ Markdown æ–‡ä»¶å¯¼å…¥åˆ° Qdrant çš„ eye_ana_guide collection

é‡‡ç”¨é€’å½’åˆ†å—ç­–ç•¥ï¼ŒæŒ‰ç…§ Markdown æ ‡é¢˜å±‚çº§ä»é«˜åˆ°ä½ä¾æ¬¡å°è¯•åˆ†å‰²ï¼š
1. ä¸€çº§æ ‡é¢˜ `# ` â†’ æŒ‰ç« èŠ‚åˆ†å—
2. äºŒçº§æ ‡é¢˜ `## ` â†’ ç« èŠ‚è¿‡é•¿æ—¶ï¼ŒæŒ‰å­ç« èŠ‚åˆ†å—
3. ä¸‰çº§æ ‡é¢˜ `### ` â†’ å­ç« èŠ‚è¿‡é•¿æ—¶ï¼ŒæŒ‰å°èŠ‚åˆ†å—
4. æ®µè½ `\n\n` â†’ å°èŠ‚è¿‡é•¿æ—¶ï¼ŒæŒ‰æ®µè½åˆ†å—
5. å¥å­ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰â†’ æ®µè½è¿‡é•¿æ—¶ï¼ŒæŒ‰å¥å­åˆ†å—
6. Token çº§åˆ« â†’ æœ€åå…œåº•
"""
import asyncio
import logging
import re
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
import os

from qdrant_client import AsyncQdrantClient, models
from qdrant_client.models import VectorParams, Distance, PointStruct, SparseVectorParams, Modifier, SparseVector
from openai import AsyncOpenAI
from chonkie import RecursiveChunker, RecursiveRules, RecursiveLevel
import jieba
import hashlib
import uuid

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
COLLECTION_NAME = "eye_ana_guide"
DATA_DIR = Path(r"C:\Users\Administrator\Desktop\eyesight_ai_counselor\data")
# ä¸­æ–‡ä¸“ä¸šæ–‡æ¡£ï¼Œchunk_size è®¾ä¸º 800-1024 tokens
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1024"))
# æŒ‰æ ‡é¢˜åˆ†å—æ—¶ï¼Œå„ç« èŠ‚ç›¸äº’ç‹¬ç«‹ï¼Œæ— éœ€é‡å 
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "0"))
# é¿å…äº§ç”Ÿè¿‡å°çš„å­¤ç«‹å—
MIN_CHARACTERS_PER_CHUNK = int(os.getenv("MIN_CHARACTERS_PER_CHUNK", "100"))
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
EMBEDDING_DIMENSIONS = int(os.getenv("EMBEDDING_DIMENSIONS", "1536"))


class DataIngestion:
    """æ•°æ®å¯¼å…¥ç±»"""

    def __init__(self):
        # åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯
        self.qdrant_client = AsyncQdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
            prefer_grpc=os.getenv("QDRANT_PREFER_GRPC", "true").lower() == "true"
        )

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.openai_client = AsyncOpenAI(
            api_key=os.getenv("EMBEDDING_API_KEY"),
            base_url=os.getenv("EMBEDDING_BASE_URL", "https://api.openai.com/v1")
        )

        # å®šä¹‰ Markdown é€’å½’åˆ†å—è§„åˆ™
        # æŒ‰ç…§æ–‡æ¡£æ–¹æ¡ˆï¼Œä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼š
        # ä¸€çº§æ ‡é¢˜ â†’ äºŒçº§æ ‡é¢˜ â†’ ä¸‰çº§æ ‡é¢˜ â†’ æ®µè½ â†’ å¥å­ â†’ Token
        markdown_rules = RecursiveRules(
            levels=[
                # ä¸€çº§æ ‡é¢˜ï¼šæŒ‰ç« èŠ‚åˆ†å—
                RecursiveLevel(delimiters=["\n# "], include_delim="next"),
                # äºŒçº§æ ‡é¢˜ï¼šç« èŠ‚è¿‡é•¿æ—¶ï¼ŒæŒ‰å­ç« èŠ‚åˆ†å—
                RecursiveLevel(delimiters=["\n## "], include_delim="next"),
                # ä¸‰çº§æ ‡é¢˜ï¼šå­ç« èŠ‚è¿‡é•¿æ—¶ï¼ŒæŒ‰å°èŠ‚åˆ†å—
                RecursiveLevel(delimiters=["\n### "], include_delim="next"),
                # æ®µè½ï¼šå°èŠ‚è¿‡é•¿æ—¶ï¼ŒæŒ‰æ®µè½åˆ†å—
                RecursiveLevel(delimiters=["\n\n"], include_delim="prev"),
                # å¥å­ï¼šæ®µè½è¿‡é•¿æ—¶ï¼ŒæŒ‰å¥å­åˆ†å—ï¼ˆä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰
                RecursiveLevel(delimiters=["ã€‚", "ï¼", "ï¼Ÿ", ". ", "! ", "? "], include_delim="prev"),
                # Token çº§åˆ«å…œåº•ï¼šæŒ‰ç©ºç™½å­—ç¬¦åˆ†å‰²
                RecursiveLevel(whitespace=True)
            ]
        )

        # åˆå§‹åŒ– RecursiveChunker åˆ†å—å™¨
        self.chunker = RecursiveChunker(
            chunk_size=CHUNK_SIZE,
            rules=markdown_rules,
            min_characters_per_chunk=MIN_CHARACTERS_PER_CHUNK
        )

        logger.info("æ•°æ®å¯¼å…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"åˆ†å—å‚æ•°: chunk_size={CHUNK_SIZE}, min_characters_per_chunk={MIN_CHARACTERS_PER_CHUNK}")
    
    async def ensure_collection_exists(self):
        """ç¡®ä¿ collection å­˜åœ¨"""
        collections = await self.qdrant_client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if COLLECTION_NAME not in collection_names:
            logger.info(f"åˆ›å»º collection: {COLLECTION_NAME}")
            await self.qdrant_client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config={
                    "dense": VectorParams(
                        size=EMBEDDING_DIMENSIONS,
                        distance=Distance.COSINE
                    )
                },
                sparse_vectors_config={
                    "bm25": SparseVectorParams(
                        modifier=Modifier.IDF
                    )
                }
            )
            logger.info(f"Collection {COLLECTION_NAME} åˆ›å»ºæˆåŠŸ")
        else:
            logger.info(f"Collection {COLLECTION_NAME} å·²å­˜åœ¨")
    
    def preprocess_markdown(self, content: str) -> str:
        """
        é¢„å¤„ç† Markdown æ–‡æ¡£ï¼Œç¡®ä¿æ ¼å¼ç»Ÿä¸€

        é¢„å¤„ç†æ­¥éª¤ï¼š
        1. ç»Ÿä¸€æ¢è¡Œç¬¦ï¼šå°† \\r\\n å’Œ \\r ç»Ÿä¸€è½¬æ¢ä¸º \\n
        2. å‹ç¼©ç©ºè¡Œï¼šå°† 3 ä¸ªåŠä»¥ä¸Šè¿ç»­æ¢è¡Œå‹ç¼©ä¸º 2 ä¸ª
        3. ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼šç¡®ä¿ # åæœ‰ç©ºæ ¼
        4. å»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½ï¼šæ¯è¡Œ strip å¤„ç†
        """
        # 1. ç»Ÿä¸€æ¢è¡Œç¬¦
        content = content.replace('\r\n', '\n').replace('\r', '\n')

        # 2. ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼šç¡®ä¿ # åæœ‰ç©ºæ ¼
        # åŒ¹é…è¡Œé¦–çš„ 1-6 ä¸ª # å·åé¢ç´§è·Ÿéç©ºæ ¼é # çš„å­—ç¬¦
        content = re.sub(r'^(#{1,6})([^\s#])', r'\1 \2', content, flags=re.MULTILINE)

        # 3. å»é™¤æ¯è¡Œé¦–å°¾ç©ºç™½ï¼Œä½†ä¿ç•™ç©ºè¡Œ
        lines = content.split('\n')
        lines = [line.strip() for line in lines]
        content = '\n'.join(lines)

        # 4. å‹ç¼©ç©ºè¡Œï¼šå°† 3 ä¸ªåŠä»¥ä¸Šè¿ç»­æ¢è¡Œå‹ç¼©ä¸º 2 ä¸ª
        content = re.sub(r'\n{3,}', '\n\n', content)

        return content.strip()

    def chunk_text(self, text: str, filename: str = "") -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ RecursiveChunker åˆ†å—æ–‡æœ¬

        è¿”å›åŒ…å«å…ƒæ•°æ®çš„ chunk åˆ—è¡¨
        """
        # é¢„å¤„ç†æ–‡æ¡£
        text = self.preprocess_markdown(text)

        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦è¿‡çŸ­
        if len(text) < 500:
            logger.warning(f"âš ï¸  æ–‡æ¡£å†…å®¹è¿‡çŸ­ï¼ˆ< 500 å­—ç¬¦ï¼‰ï¼Œæ•´ä½“ä½œä¸ºå•ä¸ª chunk: {filename}")
            return [{
                "text": text,
                "token_count": len(text),
                "chunk_index": 0,
                "start_index": 0,
                "end_index": len(text)
            }]

        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not text.strip():
            logger.error(f"âŒ ç©ºæ–‡ä»¶ï¼Œè·³è¿‡: {filename}")
            return []

        # ä½¿ç”¨ RecursiveChunker åˆ†å—
        chonks = self.chunker(text)

        chunks = []
        for idx, c in enumerate(chonks):
            chunk_text = getattr(c, "text", "").strip()
            if chunk_text:
                chunks.append({
                    "text": chunk_text,
                    "token_count": getattr(c, "token_count", len(chunk_text)),
                    "chunk_index": idx,
                    "start_index": getattr(c, "start_index", 0),
                    "end_index": getattr(c, "end_index", len(chunk_text))
                })

        return chunks
    
    def build_sparse_vector(self, text: str) -> Dict[str, Any]:
        """æ„å»º BM25 ç¨€ç–å‘é‡"""
        if not text.strip():
            return {"indices": [], "values": []}
        
        tokens = list(jieba.cut(text.strip()))
        token_freq = {}
        for token in tokens:
            if token.strip() and len(token.strip()) > 0:
                token_freq[token] = token_freq.get(token, 0) + 1
        
        indices = []
        values = []
        for token, freq in token_freq.items():
            token_bytes = token.encode('utf-8')
            token_hash = int(hashlib.md5(token_bytes).hexdigest()[:8], 16)
            indices.append(token_hash)
            values.append(float(freq))
        
        return {"indices": indices, "values": values}
    
    async def embed_text(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        response = await self.openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text.strip()
        )
        return response.data[0].embedding
    
    async def check_file_exists(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨äº collection ä¸­"""
        try:
            scroll_result = await self.qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                scroll_filter=models.Filter(
                    must=[models.FieldCondition(
                        key="filename",
                        match=models.MatchValue(value=filename)
                    )]
                ),
                limit=1,
                with_payload=True,
                with_vectors=False
            )
            return len(scroll_result[0]) > 0
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    async def ingest_file(self, file_path: Path):
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        filename = file_path.name
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if await self.check_file_exists(filename):
            logger.info(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
            return 0
        
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        # åˆ†å—
        chunks = self.chunk_text(content)
        logger.info(f"   åˆ†å—æ•°é‡: {len(chunks)}")
        
        # å¤„ç†æ¯ä¸ªåˆ†å—
        points = []
        for idx, chunk in enumerate(chunks):
            # æå–æ–‡æœ¬å†…å®¹
            chunk_text = chunk["text"]

            # ç”Ÿæˆå‘é‡
            dense_vector = await self.embed_text(chunk_text)
            sparse_vector = self.build_sparse_vector(chunk_text)

            # åˆ›å»ºç‚¹
            point = PointStruct(
                id=str(uuid.uuid4()),
                vector={
                    "dense": dense_vector,
                    "bm25": SparseVector(
                        indices=sparse_vector["indices"],
                        values=sparse_vector["values"]
                    )
                },
                payload={
                    "content": chunk_text,
                    "filename": filename,
                    "chunk_index": idx,
                    "total_chunks": len(chunks),
                    "chunk_size": len(chunk)
                }
            )
            points.append(point)
        
        # æ‰¹é‡ä¸Šä¼ 
        await self.qdrant_client.upsert(
            collection_name=COLLECTION_NAME,
            points=points
        )
        
        logger.info(f"âœ… æˆåŠŸå¯¼å…¥: {filename} ({len(points)} ä¸ªåˆ†å—)")
        return len(points)
    
    async def ingest_all(self):
        """å¯¼å…¥æ‰€æœ‰æ–‡ä»¶"""
        await self.ensure_collection_exists()

        md_files = list(DATA_DIR.glob("*.md"))
        logger.info(f"å‘ç° {len(md_files)} ä¸ª md æ–‡ä»¶")

        total_chunks = 0
        for file_path in md_files:
            chunks_count = await self.ingest_file(file_path)
            total_chunks += chunks_count

        logger.info(f"\nğŸ‰ å¯¼å…¥å®Œæˆï¼æ€»å…±å¯¼å…¥ {total_chunks} ä¸ªæ–‡æœ¬å—")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.qdrant_client.close()
        await self.openai_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    ingestion = DataIngestion()
    try:
        await ingestion.ingest_all()
    finally:
        await ingestion.close()


if __name__ == "__main__":
    asyncio.run(main())
